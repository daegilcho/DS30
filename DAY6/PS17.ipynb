{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car test-time prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tpXfx7c-ouIX"
   },
   "source": [
    "## Loading MB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 323,
     "status": "ok",
     "timestamp": 1645490179329,
     "user": {
      "displayName": "Soobin Um",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02908022989113228894"
     },
     "user_tz": -540
    },
    "id": "9lW74qlRcELP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('mercedes_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3788, 563)\n",
      "(421, 563)\n",
      "(3788,)\n",
      "(421,)\n"
     ]
    }
   ],
   "source": [
    "# Choose categorical data columns\n",
    "cf = data.select_dtypes(include=['object']).columns\n",
    "# To change it into \"categorical\" data type\n",
    "data[cf]=data[cf].astype('category')\n",
    "# One hot encoding\n",
    "data = pd.get_dummies(data)\n",
    "# Obtain X from data (excluding 'ID' and 'y')\n",
    "X_df = data.drop(['ID','y'],axis=1)\n",
    "# Obtain y from data\n",
    "y_df = data['y']\n",
    "\n",
    "# Convert y_df into binary labels\n",
    "import numpy as np\n",
    "TF_vector= (y_df<np.median(y_df))\n",
    "y_df=TF_vector.astype(float)\n",
    "\n",
    "# Conver data frame into numpy array\n",
    "X,y = X_df.values, y_df.values\n",
    "\n",
    "# Split into train and test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "#X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.1,stratify=y)\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.1,shuffle=False)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR: Hyperparameter search via cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=LogisticRegression(max_iter=10000,\n",
       "                                                solver='liblinear'),\n",
       "                   n_iter=5,\n",
       "                   param_distributions={'C': [10, 1, 0.1, 0.01, 0.001],\n",
       "                                        'penalty': ['l1', 'l2']})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV \n",
    "\n",
    "model_LR = LogisticRegression(solver='liblinear',max_iter=10000)\n",
    "penalty_list = ['l1','l2']\n",
    "C_list = [10,1,1e-1,1e-2,1e-3]\n",
    "grid_LR = {'penalty':penalty_list,'C':C_list}\n",
    "#grid_LR = dict(penalty=penalty_list,C=C_list)\n",
    "cv_LR = RandomizedSearchCV(model_LR,grid_LR,n_iter=5,cv=5)\n",
    "cv_LR.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.02395062, 0.02944255, 0.03764901, 0.09356132, 0.1758369 ]),\n",
       " 'std_fit_time': array([0.00108053, 0.00148292, 0.00251484, 0.01141583, 0.06606092]),\n",
       " 'mean_score_time': array([0.00137501, 0.00179806, 0.00180025, 0.0013999 , 0.0017858 ]),\n",
       " 'std_score_time': array([0.00049618, 0.0004035 , 0.00039876, 0.00048803, 0.00074348]),\n",
       " 'param_penalty': masked_array(data=['l1', 'l1', 'l2', 'l2', 'l1'],\n",
       "              mask=[False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_C': masked_array(data=[0.01, 0.1, 0.1, 10, 1],\n",
       "              mask=[False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'penalty': 'l1', 'C': 0.01},\n",
       "  {'penalty': 'l1', 'C': 0.1},\n",
       "  {'penalty': 'l2', 'C': 0.1},\n",
       "  {'penalty': 'l2', 'C': 10},\n",
       "  {'penalty': 'l1', 'C': 1}],\n",
       " 'split0_test_score': array([0.86543536, 0.90501319, 0.90105541, 0.88126649, 0.90369393]),\n",
       " 'split1_test_score': array([0.84432718, 0.88918206, 0.89050132, 0.86147757, 0.88654354]),\n",
       " 'split2_test_score': array([0.83905013, 0.88522427, 0.88390501, 0.83773087, 0.86015831]),\n",
       " 'split3_test_score': array([0.80317041, 0.84544254, 0.84808454, 0.82034346, 0.84280053]),\n",
       " 'split4_test_score': array([0.84015852, 0.87978864, 0.87714663, 0.84676354, 0.86261559]),\n",
       " 'mean_test_score': array([0.83842832, 0.88093014, 0.88013858, 0.84951639, 0.87116238]),\n",
       " 'std_test_score': array([0.02005331, 0.01963263, 0.01786337, 0.02073005, 0.02141688]),\n",
       " 'rank_test_score': array([5, 1, 2, 4, 3])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_LR.cv_results_ #logs results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store logs into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store logs into csv file\n",
    "import pandas as pd \n",
    "df_LR = pd.DataFrame.from_dict(cv_LR.cv_results_,orient='columns')\n",
    "# Select columns to be stored\n",
    "columns = ['params','mean_test_score','std_test_score','rank_test_score'] \n",
    "df_LR = df_LR[columns]\n",
    "df_LR.to_csv(\"logs_LR.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_model_LR.joblib']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_LR=cv_LR.best_estimator_\n",
    "from joblib import dump\n",
    "dump(best_model_LR, 'best_model_LR.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load \"best_model_LS.joblib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8931116389548693"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import load\n",
    "loaded_model_LR = load('best_model_LR.joblib')\n",
    "loaded_model_LR.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
